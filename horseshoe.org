#+TITLE:Predict LMA from leaf spectra with regularized horseshoe prior (e.g.N < P model)
#+AUTHOR: Tedward Erker
#+email: erker@wisc.edu
#+PROPERTY: header-args:R :session *R* :cache no :results output :exports both :tangle yes :eval no
-----
Fitting n << p model.  LMA ~ hyperspectral data

see these slides
https://github.com/avehtari/modelselection_tutorial/blob/master/regularizedhorseshoe_slides.pdf

see this paper
https://arxiv.org/pdf/1707.01694.pdf

gelman blog post
http://andrewgelman.com/2017/02/14/lasso-regression-etc-stan/

* Regularized Horseshoe
** libraries
#+begin_src R
library(rstanarm)
library(readr)
library(readxl)
library(dplyr)
library(ggplot2)
library(tidyr)
library(stringr)

options(mc.cores = parallel::detectCores())

#+end_src

#+RESULTS:
#+begin_example
Loading required package: Rcpp
rstanarm (Version 2.17.2, packaged: 2017-12-20 23:59:28 UTC)
- Do not expect the default priors to remain the same in future rstanarm versions.
Thus, R scripts should specify priors explicitly, even if they are just the defaults.
- For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores())
- Plotting theme set to bayesplot::theme_default().

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union
#+end_example

** load data and look at a plot of sample
*** load data
These data come from this paper: https://doi.org/10.1890/13-2110.1.
Important note:  I don't really know the whole story behind these
data, but they should work for the purposes of exploring the use of
the horseshoe prior.

lma = leaf mass per area, a foliar trait.
x = percent reflectance at a given wavelength.  wavelengths are column
names and are in nanometers.
#+begin_src R
lma <- read_excel("data/FFT_LMA.xlsx")
colnames(lma) <- c("sample", "lma") #renames columns to be same as "x" names and simplify them
x <- read_csv("data/FFT_Spectra_and_NIT_broadleaf.csv")
cn <- colnames(x)
colnames(x) <- ifelse(!grepl("^[0-9].*",cn), cn, paste0("X",cn)) # rename columns so they don't start with numbers
x <- select(x, -nitrogen_percent) # drop nitrogen, not our target
d <- left_join(lma, x)# join data
#+end_src

#+RESULTS:
: Parsed with column specification:
: cols(
:   .default = col_double(),
:   sample = col_character()
: )
: See spec(...) for full column specifications.
: |=                                                                       |   1%|==                                                                      |   3%|===                                                                     |   4%|====                                                                    |   6%|=====                                                                   |   7%|======                                                                  |   9%|=======                                                                 |  10%|=========                                                               |  12%|=========                                                       |  13%    1 MB|==========                                                      |  15%    1 MB|===========                                                     |  16%    1 MB|============                                                    |  18%    1 MB|============                                                    |  19%    1 MB|=============                                                   |  21%    1 MB|==============                                                  |  23%    1 MB|===============                                                 |  24%    1 MB|================                                                |  26%    1 MB|=================                                               |  27%    2 MB|==================                                              |  29%    2 MB|===================                                             |  30%    2 MB|====================                                            |  32%    2 MB|=====================                                           |  33%    2 MB|======================                                          |  35%    2 MB|=======================                                         |  36%    2 MB|========================                                        |  38%    2 MB|=========================                                       |  39%    2 MB|==========================                                      |  41%    3 MB|===========================                                     |  42%    3 MB|============================                                    |  44%    3 MB|=============================                                   |  45%    3 MB|==============================                                  |  47%    3 MB|===============================                                 |  48%    3 MB|================================                                |  50%    3 MB|=================================                               |  51%    3 MB|==================================                              |  53%    4 MB|===================================                             |  54%    4 MB|====================================                            |  56%    4 MB|=====================================                           |  57%    4 MB|======================================                          |  59%    4 MB|=======================================                         |  60%    4 MB|========================================                        |  62%    4 MB|=========================================                       |  63%    4 MB|==========================================                      |  65%    4 MB|===========================================                     |  66%    5 MB|============================================                    |  68%    5 MB|=============================================                   |  70%    5 MB|==============================================                  |  71%    5 MB|===============================================                 |  73%    5 MB|================================================                |  74%    5 MB|=================================================               |  76%    5 MB|==================================================              |  77%    5 MB|===================================================             |  79%    5 MB|====================================================            |  80%    6 MB|=====================================================           |  82%    6 MB|======================================================          |  83%    6 MB|=======================================================         |  85%    6 MB|========================================================        |  86%    6 MB|=========================================================       |  88%    6 MB|==========================================================      |  89%    6 MB|===========================================================     |  91%    6 MB|============================================================    |  92%    6 MB|=============================================================   |  94%    7 MB|==============================================================  |  95%    7 MB|=============================================================== |  97%    7 MB|================================================================|  98%    7 MB|=================================================================| 100%    7 MB
: Joining, by = "sample"

drop leaves for which we have no spectra (not sure why this is so).
#+begin_src R
dc <- dplyr::filter(d, complete.cases(d))
#+end_src

#+RESULTS:

*** plot data
reshape data for plotting
#+begin_src R
dcr <- dc %>% sample_n(., 30) %>% gather(key = wv, value = refl, -sample, -lma)
dcr <- dcr %>% rowwise() %>% mutate(wv = as.numeric(str_sub(wv, 2, nchar(wv))))
#+end_src

#+RESULTS:

#+begin_src R :exports results :results graphics :file figs/test_spectra.png
plt <- ggplot(dcr, aes(x = wv, y = refl, color = lma, group = sample)) + geom_line() + theme(legend.position = c(.85,.85))
print(plt)
#+end_src

#+RESULTS:
[[file:figs/test_spectra.png]]

If you want to look at the plot in plotly
#+begin_src R
#library(plotly)
#ggplotly(p, dynamicTicks = T)
#+end_src

#+RESULTS:

most the discrimination of leaf lma appears to happen in the SWIR
(greater than ~1500nm).
Also, there are a handful of spectra in the sample of 30 that are
pretty noisy out in the SWIR.

x's are very correlated.
#+begin_src R :exports both :results graphics :file figs/cor.png :width 1500 :height 1500
library(corrplot)
a <- spread(dcr, key = wv, value = refl)
colnames(a) <- ifelse(nchar(colnames(a)) == 3, paste0("0",colnames(a)), colnames(a))  # to make alphabetic ordering work right
a[,seq(3,ncol(a),20)] %>% cor %>% corrplot(order = "alphabet", type = "upper", method = "color")
#+end_src

#+RESULTS:
[[file:figs/cor.png]]

** fitting model
#+begin_src R
dim(dc)
#+end_src

#+RESULTS:
: [1]  302 2153

I'll subset the data for faster fitting during the exploratory phase

#+begin_src R
n <- 30 # number of observations to sample
p <- 100 # number of wavelengths to sample
rows <- sample(1:nrow(dc),n)
cols <- sample(3:ncol(dc),p)
dcs <- dc[rows, c(2,cols)]  # make sure I keep column 2, which contains the target response, lma.
#+end_src

#+RESULTS:

#+begin_src R
dim(dcs)
#+end_src

#+RESULTS:
: [1]  30 101

center the response, lma
#+begin_src R
dcs$lma <- dcs$lma - mean(dcs$lma)
#+end_src

#+RESULTS:

caluculate \tau_0
#+begin_src R
  p0 <- 4 # prior guess for the number of non zero coefficients
  sigmaguess <- 5
  tau0 <- p0 / (p - p0) * sigmaguess/sqrt(n)
#+end_src

#+RESULTS:

#+begin_src R
fit <- stan_glm(lma ~ ., data = dcs, gaussian(), prior = hs(global_scale=tau0), prior_intercept = normal())
#+end_src

#+RESULTS:
#+begin_example

SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).

Gradient evaluation took 0.000421 seconds
1000 transitions using 10 leapfrog steps per transition would take 4.21 seconds.
Adjust your expectations accordingly!


Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).

Gradient evaluation took 0.000314 seconds
1000 transitions using 10 leapfrog steps per transition would take 3.14 seconds.
Adjust your expectations accordingly!


Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).

Gradient evaluation took 0.000431 seconds
1000 transitions using 10 leapfrog steps per transition would take 4.31 seconds.
Adjust your expectations accordingly!


Iteration:    1 / 2000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).

Gradient evaluation took 0.000383 seconds
1000 transitions using 10 leapfrog steps per transition would take 3.83 seconds.
Adjust your expectations accordingly!


Iteration:    1 / 2000 [  0%]  (Warmup)
Iteration:  200 / 2000 [ 10%]  (Warmup)
Iteration:  200 / 2000 [ 10%]  (Warmup)
Iteration:  400 / 2000 [ 20%]  (Warmup)
Iteration:  200 / 2000 [ 10%]  (Warmup)
Iteration:  400 / 2000 [ 20%]  (Warmup)
Iteration:  600 / 2000 [ 30%]  (Warmup)
Iteration:  400 / 2000 [ 20%]  (Warmup)
Iteration:  200 / 2000 [ 10%]  (Warmup)
Iteration:  800 / 2000 [ 40%]  (Warmup)
Iteration:  600 / 2000 [ 30%]  (Warmup)
Iteration:  400 / 2000 [ 20%]  (Warmup)
Iteration:  600 / 2000 [ 30%]  (Warmup)
Iteration:  600 / 2000 [ 30%]  (Warmup)
Iteration:  800 / 2000 [ 40%]  (Warmup)
Iteration: 1000 / 2000 [ 50%]  (Warmup)
Iteration: 1001 / 2000 [ 50%]  (Sampling)
Iteration:  800 / 2000 [ 40%]  (Warmup)
Iteration:  800 / 2000 [ 40%]  (Warmup)
Iteration: 1000 / 2000 [ 50%]  (Warmup)
Iteration: 1001 / 2000 [ 50%]  (Sampling)
Iteration: 1200 / 2000 [ 60%]  (Sampling)
Iteration: 1000 / 2000 [ 50%]  (Warmup)
Iteration: 1001 / 2000 [ 50%]  (Sampling)
Iteration: 1000 / 2000 [ 50%]  (Warmup)
Iteration: 1001 / 2000 [ 50%]  (Sampling)
Iteration: 1200 / 2000 [ 60%]  (Sampling)
Iteration: 1400 / 2000 [ 70%]  (Sampling)
Iteration: 1200 / 2000 [ 60%]  (Sampling)
Iteration: 1200 / 2000 [ 60%]  (Sampling)
Iteration: 1400 / 2000 [ 70%]  (Sampling)
Iteration: 1600 / 2000 [ 80%]  (Sampling)
Iteration: 1400 / 2000 [ 70%]  (Sampling)
Iteration: 1400 / 2000 [ 70%]  (Sampling)
Iteration: 1600 / 2000 [ 80%]  (Sampling)
Iteration: 1800 / 2000 [ 90%]  (Sampling)
Iteration: 1600 / 2000 [ 80%]  (Sampling)
Iteration: 1600 / 2000 [ 80%]  (Sampling)
Iteration: 1800 / 2000 [ 90%]  (Sampling)
Iteration: 2000 / 2000 [100%]  (Sampling)

 Elapsed Time: 11.6105 seconds (Warm-up)
               12.2637 seconds (Sampling)
               23.8742 seconds (Total)

Iteration: 1800 / 2000 [ 90%]  (Sampling)
Iteration: 1800 / 2000 [ 90%]  (Sampling)
Iteration: 2000 / 2000 [100%]  (Sampling)

 Elapsed Time: 13.4465 seconds (Warm-up)
               11.9388 seconds (Sampling)
               25.3852 seconds (Total)

Iteration: 2000 / 2000 [100%]  (Sampling)

 Elapsed Time: 14.7708 seconds (Warm-up)
               11.3052 seconds (Sampling)
               26.076 seconds (Total)

Iteration: 2000 / 2000 [100%]  (Sampling)

 Elapsed Time: 14.6839 seconds (Warm-up)
               11.2151 seconds (Sampling)
               25.899 seconds (Total)
#+end_example


#+begin_src R :exports results :results graphics :file ../figs/coefs.png
plot(fit)
#+end_src

#+RESULTS:
[[file:../figs/coefs.png]]

The coefficients aren't shrunk towards zero like I would have
expected. Is this due to the nature of the data, or how I specified
the model?




